#!/usr/bin/env python3
"""
RSSè§£ææ¨¡å— - æ”¯æŒå¤šç§æ’­å®¢RSSæ ¼å¼
"""

import feedparser
import time
import hashlib
from datetime import datetime
from urllib.parse import urlparse
import requests
from typing import Dict, List, Optional, Any
import json

from config import DOWNLOAD_TIMEOUT


class RSSParserError(Exception):
    """RSSè§£æé”™è¯¯å¼‚å¸¸"""
    pass


class RSSParser:
    """RSSè§£æå™¨ - æ”¯æŒå¤šç§æ’­å®¢æ ¼å¼"""
    
    def __init__(self, timeout: int = DOWNLOAD_TIMEOUT):
        self.timeout = timeout
        self.user_agent = (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        # æ”¯æŒçš„æ’­å®¢å¹³å°è¯†åˆ«
        self.platform_patterns = {
            "apple": ["apple.com", "podcasts.apple.com", "itunes.apple.com"],
            "spotify": ["spotify.com", "open.spotify.com"],
            "google": ["google.com", "podcasts.google.com"],
            "xiaoyuzhou": ["xiaoyuzhoufm.com"],
            "getpodcast": ["getpodcast.xyz"],
            "dedao": ["dedao.cn", "igetget.com"],
        }
    
    def parse_feed(self, rss_url: str) -> Dict[str, Any]:
        """
        è§£æRSS feed
        
        Args:
            rss_url: RSS feed URL
            
        Returns:
            Dict containing feed metadata and episodes
        """
        print(f"ğŸ“¡ è§£æRSS feed: {rss_url[:80]}..." if len(rss_url) > 80 else f"ğŸ“¡ è§£æRSS feed: {rss_url}")
        
        try:
            # è§£æfeed
            feed = feedparser.parse(rss_url)
            
            if feed.bozo:
                # å°è¯•ä½¿ç”¨è‡ªå®šä¹‰User-Agent
                headers = {"User-Agent": self.user_agent}
                response = requests.get(rss_url, headers=headers, timeout=self.timeout)
                response.raise_for_status()
                feed = feedparser.parse(response.content)
            
            # æ£€æŸ¥feedæ˜¯å¦æœ‰æ•ˆ
            if not feed.entries:
                raise RSSParserError(f"RSS feedæ²¡æœ‰å†…å®¹æˆ–æ— æ³•è§£æ: {rss_url}")
            
            # æå–feedä¿¡æ¯
            feed_info = self._extract_feed_info(feed, rss_url)
            
            # æå–å‰§é›†ä¿¡æ¯
            episodes = self._extract_episodes(feed)
            
            return {
                "feed_info": feed_info,
                "episodes": episodes,
                "total_episodes": len(episodes),
                "parse_time": datetime.now().isoformat(),
                "source_url": rss_url,
            }
            
        except Exception as e:
            raise RSSParserError(f"è§£æRSSå¤±è´¥: {e}")
    
    def _extract_feed_info(self, feed: feedparser.FeedParserDict, rss_url: str) -> Dict[str, Any]:
        """æå–feedå…ƒæ•°æ®"""
        feed_info = {
            "title": feed.feed.get("title", "æœªçŸ¥æ’­å®¢"),
            "description": feed.feed.get("description", ""),
            "link": feed.feed.get("link", rss_url),
            "language": feed.feed.get("language", "zh"),
            "updated": feed.feed.get("updated", ""),
            "generator": feed.feed.get("generator", ""),
            "image_url": self._get_feed_image(feed),
            "author": feed.feed.get("author", ""),
            "rss_url": rss_url,
            "platform": self._detect_platform(rss_url),
        }
        
        # æ¸…ç†å’Œæ ‡å‡†åŒ–æ•°æ®
        feed_info["title"] = self._clean_text(feed_info["title"])
        feed_info["description"] = self._clean_text(feed_info["description"])
        
        return feed_info
    
    def _extract_episodes(self, feed: feedparser.FeedParserDict) -> List[Dict[str, Any]]:
        """æå–å‰§é›†ä¿¡æ¯"""
        episodes = []
        
        for entry in feed.entries:
            try:
                episode = self._extract_single_episode(entry)
                if episode:
                    episodes.append(episode)
            except Exception as e:
                print(f"âš ï¸  è·³è¿‡æ— æ³•è§£æçš„å‰§é›†: {e}")
                continue
        
        # æŒ‰å‘å¸ƒæ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        episodes.sort(key=lambda x: x.get("published_parsed", (1970, 1, 1)), reverse=True)
        
        return episodes
    
    def _extract_single_episode(self, entry: feedparser.FeedParserDict) -> Optional[Dict[str, Any]]:
        """æå–å•ä¸ªå‰§é›†ä¿¡æ¯"""
        # è·å–æ ‡é¢˜
        title = entry.get("title", "æœªçŸ¥æ ‡é¢˜")
        title = self._clean_text(title)
        
        if not title:
            return None
        
        # è·å–å‘å¸ƒæ—¥æœŸ
        published = entry.get("published", "")
        published_parsed = entry.get("published_parsed")
        
        # è·å–æè¿°
        description = ""
        if "summary" in entry:
            description = entry.summary
        elif "description" in entry:
            description = entry.description
        elif "content" in entry:
            # å°è¯•ä»contentä¸­æå–
            for content in entry.content:
                if hasattr(content, "value"):
                    description = content.value
                    break
        
        description = self._clean_text(description)
        
        # è·å–éŸ³é¢‘URL
        audio_url = self._find_audio_url(entry)
        
        # è·å–å‰§é›†æ—¶é•¿
        duration = self._extract_duration(entry)
        
        # è·å–å‰§é›†ç¼–å·
        episode_number = self._extract_episode_number(entry, title)
        
        # ç”Ÿæˆå”¯ä¸€ID
        episode_id = self._generate_episode_id(entry, title, audio_url)
        
        return {
            "id": episode_id,
            "title": title,
            "description": description,
            "published": published,
            "published_parsed": published_parsed,
            "audio_url": audio_url,
            "duration": duration,
            "episode_number": episode_number,
            "link": entry.get("link", ""),
            "author": entry.get("author", ""),
            "guid": entry.get("id", ""),
            "enclosures": self._get_enclosures(entry),
        }
    
    def _find_audio_url(self, entry: feedparser.FeedParserDict) -> Optional[str]:
        """æŸ¥æ‰¾éŸ³é¢‘URL"""
        # æ£€æŸ¥enclosures
        if hasattr(entry, "enclosures"):
            for enclosure in entry.enclosures:
                if enclosure.type.startswith("audio/"):
                    return enclosure.href
                elif enclosure.type.startswith("video/"):
                    # æœ‰äº›æ’­å®¢ä½¿ç”¨videoæ ¼å¼ä½†å®é™…ä¸Šæ˜¯éŸ³é¢‘
                    return enclosure.href
        
        # æ£€æŸ¥links
        if hasattr(entry, "links"):
            for link in entry.links:
                if link.type.startswith("audio/"):
                    return link.href
        
        # æ£€æŸ¥itunesæ‰©å±•
        if hasattr(entry, "itunes_duration"):
            # å¦‚æœæœ‰itunes_durationï¼Œå¯èƒ½åœ¨å…¶ä»–å­—æ®µä¸­
            pass
        
        # å°è¯•ä»descriptionä¸­æå–
        if hasattr(entry, "description"):
            import re
            desc = entry.description
            # æŸ¥æ‰¾å¸¸è§çš„éŸ³é¢‘URLæ¨¡å¼
            audio_patterns = [
                r'https?://[^\s<>"\']+\.(mp3|m4a|wav|ogg|flac|aac)[^\s<>"\']*',
                r'src=["\']([^"\']+\.(mp3|m4a|wav|ogg|flac|aac))["\']',
            ]
            
            for pattern in audio_patterns:
                matches = re.findall(pattern, desc, re.IGNORECASE)
                if matches:
                    if isinstance(matches[0], tuple):
                        return matches[0][0]
                    else:
                        return matches[0]
        
        return None
    
    def _extract_duration(self, entry: feedparser.FeedParserDict) -> Optional[str]:
        """æå–å‰§é›†æ—¶é•¿"""
        # æ£€æŸ¥itunesæ‰©å±•
        if hasattr(entry, "itunes_duration"):
            duration = entry.itunes_duration
            if duration:
                # æ ‡å‡†åŒ–æ—¶é•¿æ ¼å¼ (HH:MM:SS æˆ– MM:SS)
                parts = duration.split(":")
                if len(parts) == 3:
                    return duration  # HH:MM:SS
                elif len(parts) == 2:
                    # å¯èƒ½æ˜¯MM:SSï¼Œæ£€æŸ¥æ˜¯å¦è¶…è¿‡60åˆ†é’Ÿ
                    minutes, seconds = map(int, parts)
                    if minutes >= 60:
                        hours = minutes // 60
                        minutes = minutes % 60
                        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
                    else:
                        return f"00:{minutes:02d}:{seconds:02d}"
        
        return None
    
    def _extract_episode_number(self, entry: feedparser.FeedParserDict, title: str) -> Optional[int]:
        """æå–å‰§é›†ç¼–å·"""
        # æ£€æŸ¥itunesæ‰©å±•
        if hasattr(entry, "itunes_episode"):
            try:
                return int(entry.itunes_episode)
            except (ValueError, TypeError):
                pass
        
        # ä»æ ‡é¢˜ä¸­æå–
        import re
        patterns = [
            r'ç¬¬\s*(\d+)\s*[æœŸé›†å›]',
            r'Episode\s*(\d+)',
            r'EP\.?\s*(\d+)',
            r'#\s*(\d+)',
            r'(\d+)\s*[\.\-\s]',  # ä»¥æ•°å­—å¼€å¤´
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except (ValueError, TypeError):
                    continue
        
        return None
    
    def _generate_episode_id(self, entry: feedparser.FeedParserDict, title: str, audio_url: str) -> str:
        """ç”Ÿæˆå‰§é›†å”¯ä¸€ID"""
        # ä½¿ç”¨guidå¦‚æœå­˜åœ¨
        if hasattr(entry, "id") and entry.id:
            guid_hash = hashlib.md5(entry.id.encode()).hexdigest()[:12]
            return f"ep_{guid_hash}"
        
        # ä½¿ç”¨æ ‡é¢˜å’ŒéŸ³é¢‘URLçš„ç»„åˆ
        id_string = f"{title}_{audio_url}" if audio_url else title
        title_hash = hashlib.md5(id_string.encode()).hexdigest()[:12]
        
        # æ·»åŠ æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§
        timestamp = int(time.time())
        
        return f"ep_{title_hash}_{timestamp}"
    
    def _get_enclosures(self, entry: feedparser.FeedParserDict) -> List[Dict[str, str]]:
        """è·å–æ‰€æœ‰é™„ä»¶ä¿¡æ¯"""
        enclosures = []
        
        if hasattr(entry, "enclosures"):
            for enc in entry.enclosures:
                enclosures.append({
                    "url": enc.href,
                    "type": enc.type,
                    "length": getattr(enc, "length", None),
                })
        
        return enclosures
    
    def _get_feed_image(self, feed: feedparser.FeedParserDict) -> Optional[str]:
        """è·å–feedå›¾ç‰‡URL"""
        # æ£€æŸ¥itunesæ‰©å±•
        if hasattr(feed.feed, "image"):
            if hasattr(feed.feed.image, "href"):
                return feed.feed.image.href
        
        # æ£€æŸ¥å…¶ä»–å›¾ç‰‡å­—æ®µ
        image_fields = ["image", "logo", "icon"]
        for field in image_fields:
            if hasattr(feed.feed, field):
                value = getattr(feed.feed, field)
                if isinstance(value, str) and value.startswith("http"):
                    return value
        
        return None
    
    def _detect_platform(self, rss_url: str) -> str:
        """æ£€æµ‹æ’­å®¢å¹³å°"""
        parsed_url = urlparse(rss_url)
        domain = parsed_url.netloc.lower()
        
        for platform, patterns in self.platform_patterns.items():
            for pattern in patterns:
                if pattern in domain:
                    return platform
        
        return "unknown"
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # ç§»é™¤HTMLæ ‡ç­¾
        import re
        text = re.sub(r'<[^>]+>', ' ', text)
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = ' '.join(text.split())
        
        # é™åˆ¶é•¿åº¦
        if len(text) > 1000:
            text = text[:1000] + "..."
        
        return text.strip()
    
    def get_latest_episodes(self, rss_url: str, limit: int = 5) -> List[Dict[str, Any]]:
        """è·å–æœ€æ–°å‰§é›†"""
        feed_data = self.parse_feed(rss_url)
        return feed_data["episodes"][:limit]
    
    def check_for_new_episodes(self, rss_url: str, last_check_time: Optional[datetime] = None) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æ–°å‰§é›†"""
        feed_data = self.parse_feed(rss_url)
        
        if not last_check_time:
            return feed_data["episodes"][:5]  # è¿”å›æœ€æ–°çš„5ä¸ª
        
        new_episodes = []
        for episode in feed_data["episodes"]:
            published = episode.get("published_parsed")
            if published:
                # å°†published_parsedè½¬æ¢ä¸ºdatetime
                from time import mktime
                from datetime import datetime as dt
                episode_time = dt.fromtimestamp(mktime(published))
                
                if episode_time > last_check_time:
                    new_episodes.append(episode)
        
        return new_episodes


# ä¾¿æ·å‡½æ•°
def parse_rss_feed(rss_url: str) -> Dict[str, Any]:
    """è§£æRSS feedçš„ä¾¿æ·å‡½æ•°"""
    parser = RSSParser()
    return parser.parse_feed(rss_url)


def get_latest_episodes(rss_url: str, limit: int = 5) -> List[Dict[str, Any]]:
    """è·å–æœ€æ–°å‰§é›†çš„ä¾¿æ·å‡½æ•°"""
    parser = RSSParser()
    return parser.get_latest_episodes(rss_url, limit)


def save_feed_to_json(feed_data: Dict[str, Any], filepath: str) -> None:
    """ä¿å­˜feedæ•°æ®åˆ°JSONæ–‡ä»¶"""
    import json
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(feed_data, f, ensure_ascii=False, indent=2, default=str)


def load_feed_from_json(filepath: str) -> Dict[str, Any]:
    """ä»JSONæ–‡ä»¶åŠ è½½feedæ•°æ®"""
    import json
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import argparse
    
    parser = argparse.ArgumentParser(description="RSSè§£ææµ‹è¯•")
    parser.add_argument("--url", required=True, help="RSS feed URL")
    parser.add_argument("--limit", type=int, default=3, help="æ˜¾ç¤ºæœ€æ–°å‡ æœŸ")
    parser.add_argument("--save", help="ä¿å­˜åˆ°JSONæ–‡ä»¶")
    
    args = parser.parse_args()
    
    try:
        print("=" * 60)
        print("ğŸ“¡ RSSè§£ææµ‹è¯•")
        print("=" * 60)
        
        # è§£æfeed
        feed_data = parse_rss_feed(args.url)
        
        # æ˜¾ç¤ºfeedä¿¡æ¯
        feed_info = feed_data["feed_info"]
        print(f"\nğŸ™ï¸  æ’­å®¢ä¿¡æ¯:")
        print(f"  æ ‡é¢˜: {feed_info['title']}")
        print(f"  æè¿°: {feed_info['description'][:100]}..." if len(feed_info['description']) > 100 else f"  æè¿°: {feed_info['description']}")
        print(f"  å¹³å°: {feed_info['platform']}")
        print(f"  è¯­è¨€: {feed_info['language']}")
        print(f"  æ€»æœŸæ•°: {feed_data['total_episodes']}")
        
        # æ˜¾ç¤ºæœ€æ–°å‰§é›†
        print(f"\nğŸ“‹ æœ€æ–° {args.limit} æœŸ:")
        for i, episode in enumerate(feed_data["episodes"][:args.limit], 1):
            print(f"\n  {i}. {episode['title']}")
            print(f"     å‘å¸ƒæ—¥æœŸ: {episode['published']}")
            if episode['duration']:
                print(f"     æ—¶é•¿: {episode['duration']}")
            if episode['episode_number']:
                print(f"     æœŸå·: {episode['episode_number']}")
            if episode['audio_url']:
                print(f"     éŸ³é¢‘: {episode['audio_url'][:80]}..." if len(episode['audio_url']) > 80 else f"     éŸ³é¢‘: {episode['audio_url']}")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        if args.save:
            save_feed_to_json(feed_data, args.save)
            print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {args.save}")
        
        print("\n" + "=" * 60)
        print("âœ… RSSè§£ææµ‹è¯•å®Œæˆ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ RSSè§£æå¤±è´¥: {e}")
        print("\nğŸ’¡ æ•…éšœæ’é™¤:")
        print("1. æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. å°è¯•ä½¿ç”¨ä»£ç†")
        print("4. ç¡®è®¤RSS feedå¯å…¬å¼€è®¿é—®")